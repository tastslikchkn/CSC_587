---
title: "CSC 587 HW 4"
author: "Daniel R. Getty"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: 
    css: "style.css"
    includes:
      in_header: logo.html
---

```{r rsetup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, results='asis')
```

```{python psetup, include=FALSE}
import os
import pandas as pd
import numpy as np
import scipy.stats as stats
import math
from matplotlib import pyplot as plt
```

## Homework 4

1. Given a data tuple having the values ”systems”, ”26_30”, and ”46K_50K” for the attributes department, age, and salary,
respectively, what would a naive Bayesian classification of the status according to the data above? Notice that Count
column is NOT an attribute. It just tells how many times a row occurs in our database and status is our target variable.

```{python Problem 1 Data, include=TRUE}
# create a data frame
data = {'department': ['sales', 'sales', 'sales', 'systems', 'systems', 'systems', 'systems', 'marketing', 'marketing', 'secretary', 'secretary'],'age': ['31_35', '26_30', '31_35', '21_25', '31_35', '26_30', '41_45', '36_40', '31_35', '46_50', '26_30'],'salary': ['46K_50K', '26K_30K', '31K_35K', '46K_50K', '66K_70K', '46K_50K', '66K_70K', '46K_50K', '41K_45K', '36K_40K', '26K_30K'], 'status': ['senior', 'junior', 'junior', 'junior', 'senior', 'junior', 'senior', 'senior', 'junior', 'senior', 'junior'],'count': [30, 40, 40, 20, 5, 3, 3, 10, 4, 4, 6]}

df = pd.DataFrame(data)
print(df.to_markdown(tablefmt="grid"))
```



```{python Problem 1, include=TRUE}

# df.head()
# print (df)

# calculate the probability of each status
sum_total = sum(df['count'])
print("Total:",sum_total)

senior_total = sum(df['count'][df['status'] == 'senior'])
print("Senior Total:",senior_total)

junior_total = sum(df['count'][df['status'] == 'junior'])
print("Junior Total:",junior_total)

PStaus_Senior = (senior_total / sum_total)

print("P(Staus = Senior)",round(PStaus_Senior,2))

PStaus_Junior = (junior_total / sum_total)

print("P(Staus = Junior)",round(PStaus_Junior,2))

# calculate the probability of systems department
systems_total = sum(df['count'][df['department'] == 'systems'])
print("Department Total:",systems_total)

PDept_Systems = (systems_total / sum_total)
print("P(Department = Systems)",round(PDept_Systems,2))

# calculate the probability of age 26_30
age_total = sum(df['count'][df['age'] == '26_30'])
print("Age Total:",age_total)

PAge_26_30 = (age_total / sum_total)
print("P(Age = 26_30)",round(PAge_26_30,2))

# calculate the probability of salary 46K_50K
salary_total = sum(df['count'][df['salary'] == '46K_50K'])
print("Salary Total:",salary_total)

PSalary_46K_50K = (salary_total / sum_total)
print("P(Salary = 46K_50K)",round(PSalary_46K_50K,2))

# calculate the probability of status senior given department systems, age 26_30, and salary 46K_50K
PStatus_Senior_x = (PDept_Systems * PAge_26_30 * PSalary_46K_50K * PStaus_Senior)
print("P(Status = Senior | Department = Systems, Age = 26_30, Salary = 46K_50K)",round(PStatus_Senior_x,2))

# calculate the probability of status junior given department systems, age 26_30, and salary 46K_50K
PStatus_Junior_x = (PDept_Systems * PAge_26_30 * PSalary_46K_50K * PStaus_Junior)
print("P(Status = Junior | Department = Systems, Age = 26_30, Salary = 46K_50K)",round(PStatus_Junior_x,2))
```


2. split your diabetes data into two parts for training and testing purposes. Namely, reserve last 10 rows of the diabetes_train.csv for the test set. Then fit a SVM classifier on the bigger portion of this data and test it on these 10 rows you had reserved. Please feel free to modify existing codes. Notice that you’re not going to read diabetes_test.csv anymore since you’re going to split the bigger data. Please submit your Python code and your prediction results

```{python Problem 2, include=TRUE}
# read the diabetes data
diabetes = pd.read_csv('diabetes_train.csv')

# split the data into training and testing reserves last 10 rows for testing
test = diabetes.iloc[-10:]
train = diabetes.iloc[:-10]

print("Test Data")
print(test.to_markdown(tablefmt="grid"))

print("\n")
print("Train Data")
dfhead = train.head()
print(dfhead.to_markdown(tablefmt="grid"))
print("\n")
# print(train.to_markdown(tablefmt="grid"))

# fit a SVM classifier
from sklearn import svm
clf = svm.SVC()
clf.fit(train.iloc[:,:-1], train.iloc[:,-1])

# test the classifier
prediction = clf.predict(test.iloc[:,:-1])
prediction = pd.DataFrame(prediction, columns=['Prediction'])
# print the prediction results
# print("Predicted Results =",prediction)
print(prediction.to_markdown())




```

3.   
    
```{python Problem 3, include=TRUE}
# create a data frame
classes = ['a','b','c','d','e','f','g','h']
data = {'x1': [2, 2, 8, 5, 7, 6, 1, 4],'x2': [10, 5, 4, 8, 5, 4, 2, 9]}
P3_df = pd.DataFrame(data, index=classes)
# print(P3_df)
print(P3_df.to_markdown(tablefmt="grid"))



# build a scatterplot of the df
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'])
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))

plt.show()
```
(a) If h and c are selected as the initial centers for your k-means clustering, assign memberships for other points, and
compute the means (centroids) of your initial clusters. You can use Manhattan distance.

```{python Problem 3a, include=TRUE}
# initial centers
c1 = P3_df.loc['h']
c2 = P3_df.loc['c']
print(c1)
print(c2)

# assign memberships
P3_df['d_c1'] = np.sqrt((P3_df['x1'] - c1['x1'])**2 + (P3_df['x2'] - c1['x2'])**2)
P3_df['d_c2'] = np.sqrt((P3_df['x1'] - c2['x1'])**2 + (P3_df['x2'] - c2['x2'])**2)
P3_df['mem'] = np.where(P3_df['d_c1'] < P3_df['d_c2'], 'c1', 'c2')

# print(P3_df)
print(P3_df.to_markdown(tablefmt="grid"))

# build a visualization of the k-means clustering
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'], c=P3_df['mem'].map({'c1': 'red', 'c2': 'blue'}))
plt.scatter(c1['x1'], c1['x2'], color='red', marker='x', s=100)
plt.scatter(c2['x1'], c2['x2'], color='blue', marker='x', s=100)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2 with k-means clustering')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))

plt.show()

```

(b) Based on the centroids you found above reassign the memberships by using Manhattan distance

```{python Problem 3b, include=TRUE}
# reassign memberships
c1 = P3_df[P3_df['mem'] == 'c1'][['x1', 'x2']].mean()
c2 = P3_df[P3_df['mem'] == 'c2'][['x1', 'x2']].mean()


P3_df['d_c1'] = np.sqrt((P3_df['x1'] - c1['x1'])**2 + (P3_df['x2'] - c1['x2'])**2)
P3_df['d_c2'] = np.sqrt((P3_df['x1'] - c2['x1'])**2 + (P3_df['x2'] - c2['x2'])**2)
P3_df['mem'] = np.where(P3_df['d_c1'] < P3_df['d_c2'], 'c1', 'c2')

# print(P3_df)
print(P3_df.to_markdown(tablefmt="grid"))

# build a visualization of the reassigned k-means clustering
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'], c=P3_df['mem'].map({'c1': 'red', 'c2': 'blue'}))
plt.scatter(c1['x1'], c1['x2'], color='red', marker='x', s=100)
plt.scatter(c2['x1'], c2['x2'], color='blue', marker='x', s=100)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2 with reassigned k-means clustering')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))
    
plt.show()
```


4. Given the distance matrix below answer the following questions. Notice that this is a distance matrix, meaning the distance between any pair of points can be found by checking the corresponding cell.

```{python Problem 4, include=TRUE}
# create a data frame
classes = ['b','c','d','e','f','g','h']
data = {'a': [5, 8, 4, 7, 7, 8, 2],'b': [0, 6, 4, 5, 4, 3, 4],'c': [0, 0, 5, 1, 2, 7, 6],'d': [0, 0, 0, 4, 4, 7, 1],'e': [0,0, 0, 0, 1, 7, 5],'f': [0, 0, 0, 0, 0, 5, 5],'g': [0, 0, 0, 0, 0, 0, 8]}
P4_df = pd.DataFrame(data, index=classes)
# print(P4_df)
print(P4_df.to_markdown(tablefmt="grid"))

```

(a) Apply the hierarchical clustering algorithm with single linkage to the data above. Draw the final dendrogram. 

```{python Problem 4a, include=TRUE}
# apply hierarchical clustering with single linkage
from scipy.cluster.hierarchy import linkage, dendrogram
Z = linkage(P4_df, 'single')
plt.figure(figsize=(10, 6))
dendrogram(Z, p=30, truncate_mode=None, color_threshold=None, get_leaves=True, orientation='top', labels=classes, count_sort=False, distance_sort=False, show_leaf_counts=True, no_plot=False, no_labels=False, leaf_font_size=None, leaf_rotation=None, leaf_label_func=None, show_contracted=False, link_color_func=None, ax=None, above_threshold_color='C0')
plt.title('Dendrogram of hierarchical clustering with single linkage')
plt.show()
```

(b) Determine whether a point is core based on eps = 6 and 'minPts' = 2. (Recall that a point p is a core point if at least 'minPts' points are within distance eps of it (including p).

```{python Problem 4b, include=TRUE}
# determine core points
eps = 6
minPts = 2
core_points = []
for i in range(len(P4_df)):
    if sum(P4_df.iloc[i] <= eps) >= minPts:
        core_points.append(classes[i])
        
print("Core Points:",core_points)
```