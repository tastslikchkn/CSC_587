---
title: "CSC 587 MiniProjecty"
author: "Daniel R. Getty"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: 
    css: "style.css"
    includes:
      in_header: logo.html
---

```{r rsetup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, results='asis')
# directory
# Print the working directory.
getwd()
# load ggplot2 package
library(ggplot2)
# load ggplot2 package
library(ggplot2)
# load dplyr package
library(dplyr)
# load tidyr package
library(tidyr)
```


```{python psetup, include=FALSE}
import os
import pandas as pd
import numpy as np
import scipy.stats as stats
import math
from matplotlib import pyplot as plt
```


```{python Problem 1, include=TRUE}


```

2. split your diabetes data into two parts for training and testing purposes. Namely, reserve last 10 rows of the diabetes_train.csv for the test set. Then fit a SVM classifier on the bigger portion of this data and test it on these 10 rows you had reserved. Please feel free to modify existing codes. Notice that you’re not going to read diabetes_test.csv anymore since you’re going to split the bigger data. Please submit your Python code and your prediction results

```{python Problem 2, include=TRUE}
# read the diabetes data
diabetes = pd.read_csv('diabetes_train.csv')

# split the data into training and testing reserves last 10 rows for testing
test = diabetes.iloc[-10:]
train = diabetes.iloc[:-10]

print(test)
print(train)


# fit a SVM classifier
from sklearn import svm
clf = svm.SVC()
clf.fit(train.iloc[:,:-1], train.iloc[:,-1])

# test the classifier
prediction = clf.predict(test.iloc[:,:-1])

# print the prediction results
print("Predicted Results =",prediction)
```

3.   
    
```{python Problem 3, include=TRUE}
# create a data frame
classes = ['a','b','c','d','e','f','g','h']
data = {'x1': [2, 2, 8, 5, 7, 6, 1, 4],'x2': [10, 5, 4, 8, 5, 4, 2, 9]}
P3_df = pd.DataFrame(data, index=classes)
print(P3_df)

# build a scatterplot of the df
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'])
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))

plt.show()
```
(a) If h and c are selected as the initial centers for your k-means clustering, assign memberships for other points, and
compute the means (centroids) of your initial clusters. You can use Manhattan distance.

```{python Problem 3a, include=TRUE}
# initial centers
c1 = P3_df.loc['h']
c2 = P3_df.loc['c']
print(c1)
print(c2)

# assign memberships
P3_df['d_c1'] = np.sqrt((P3_df['x1'] - c1['x1'])**2 + (P3_df['x2'] - c1['x2'])**2)
P3_df['d_c2'] = np.sqrt((P3_df['x1'] - c2['x1'])**2 + (P3_df['x2'] - c2['x2'])**2)
P3_df['mem'] = np.where(P3_df['d_c1'] < P3_df['d_c2'], 'c1', 'c2')

print(P3_df)

# build a visualization of the k-means clustering
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'], c=P3_df['mem'].map({'c1': 'red', 'c2': 'blue'}))
plt.scatter(c1['x1'], c1['x2'], color='red', marker='x', s=100)
plt.scatter(c2['x1'], c2['x2'], color='blue', marker='x', s=100)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2 with k-means clustering')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))

plt.show()

```

(b) Based on the centroids you found above reassign the memberships by using Manhattan distance

```{python Problem 3b, include=TRUE}
# reassign memberships
c1 = P3_df[P3_df['mem'] == 'c1'][['x1', 'x2']].mean()
c2 = P3_df[P3_df['mem'] == 'c2'][['x1', 'x2']].mean()


P3_df['d_c1'] = np.sqrt((P3_df['x1'] - c1['x1'])**2 + (P3_df['x2'] - c1['x2'])**2)
P3_df['d_c2'] = np.sqrt((P3_df['x1'] - c2['x1'])**2 + (P3_df['x2'] - c2['x2'])**2)
P3_df['mem'] = np.where(P3_df['d_c1'] < P3_df['d_c2'], 'c1', 'c2')

print(P3_df)

# build a visualization of the reassigned k-means clustering
plt.figure(figsize=(10, 6))
plt.scatter(P3_df['x1'], P3_df['x2'], c=P3_df['mem'].map({'c1': 'red', 'c2': 'blue'}))
plt.scatter(c1['x1'], c1['x2'], color='red', marker='x', s=100)
plt.scatter(c2['x1'], c2['x2'], color='blue', marker='x', s=100)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatterplot of x1 and x2 with reassigned k-means clustering')
for i, txt in enumerate(classes):
    plt.annotate(txt, (P3_df['x1'][i], P3_df['x2'][i]))
    
plt.show()
```


4. Given the distance matrix below answer the following questions. Notice that this is a distance matrix, meaning the distance between any pair of points can be found by checking the corresponding cell.

```{python Problem 4, include=TRUE}
# create a data frame
classes = ['b','c','d','e','f','g','h']
data = {'a': [5, 8, 4, 7, 7, 8, 2],'b': [0, 6, 4, 5, 4, 3, 4],'c': [0, 0, 5, 1, 2, 7, 6],'d': [0, 0, 0, 4, 4, 7, 1],'e': [0,0, 0, 0, 1, 7, 5],'f': [0, 0, 0, 0, 0, 5, 5],'g': [0, 0, 0, 0, 0, 0, 8]}
P4_df = pd.DataFrame(data, index=classes)
print(P4_df)


```

(a) Apply the hierarchical clustering algorithm with single linkage to the data above. Draw the final dendrogram. 

```{python Problem 4a, include=TRUE}
# apply hierarchical clustering with single linkage
from scipy.cluster.hierarchy import linkage, dendrogram
Z = linkage(P4_df, 'single')
plt.figure(figsize=(10, 6))
dendrogram(Z, labels=classes)
plt.title('Dendrogram of hierarchical clustering with single linkage')
plt.show()

```

(b) Determine whether a point is core based on eps = 6 and 'minPts' = 2. (Recall that a point p is a core point if at least 'minPts' points are within distance eps of it (including p).

```{python Problem 4b, include=TRUE}
# determine core points
eps = 6
minPts = 2
core_points = []
for i in range(len(P4_df)):
    if sum(P4_df.iloc[i] <= eps) >= minPts:
        core_points.append(classes[i])
        
print("Core Points:",core_points)


```